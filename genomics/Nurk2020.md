# Article  
HiCanu: accurate assembly of segmental duplications, satellites, and allelic variants from high-fidelity long reads ([link here](https://doi.org/10.1101/gr.263566.120))  

# Authors  
* Sergey Nurk
* Brian P. Walenz
* Arang Rhie
* Mitchell R. Vollger
* Glennis A. Logsdon
* Robert Grothe
* Karen H. Miga
* Evan E. Eichler
* Adam M. Phillippy
* Sergey Koren

# Journal and Year  
Genome Research (2020)  

# Highlights  
## 1. Advantages of HiFi data  
* Although HiFi reads are shorter (up to 25 Kb) than the traditional CLR reads (that can exceed 50 Kb) or the Nanopore reads (that can exceed 100 Kb), HiFi reads are much more accurate compared to other long-read technologies. While the median accuracy of the former can exceed 99.9% (>Q30), the accuracy of the latter do not exceed 95%. 
## 2. HiCanu vs. Canu  
* HiCanu was built on the original Canu pipeline, which was developed for dealing with conventional long-read data, but it is not optimized to deal with HiFi data. HiCanu was built after replacement and significant modification of Canu's core methods in order to better deal with HiFi data. Here are some key steps in the HiCanu pipeline:  
    1. HiCanu begins by compressing consecutive copies of the same nucleotide (homopolymers) to a single base (e.g. "AA..." becomes "A").  
        * This is done because homopolymer length is the primary error mode of HiFi technology  
    2. The compressed reads are trimmed based on their overlaps to other reads to remove any chimeric sequences or sequencing adapters, and the overlaps are recomputed based on the trimmed reads
    3. The Overlap Error Adjustment (OEA) module examines read overlap pileups to identify remaining sequencing errors in the individual reads and recomputes overlap alignment identities. Here, a modification was applied to the OEA module to ignore any differences within microsatellite repeat arrays due to the fact that they are also prone to HiFi read errors. Then two reads that differ only by an indel detected in a microsatellite region are considered to be 100% identical.   
    <img src="https://user-images.githubusercontent.com/22843614/92035046-826d9480-ed44-11ea-8427-316d65319521.png" width=30%></img>  
    *Figure 1. A) Two hypothetical reads are shown with sequencing errors highlighted in red. B) The first step of HiCanu is to compress homopolymers, which obscures homopolymer length errors but retains enough information to accurately distinguish reads from different genomic loci. C) Overlaps are then computed for the compressed reads, and remaining errors are identified by examining the alignment pileups (gray rectangle). D) Finally, after correcting the identified errors (blue) and ignoring indels in regions of known systematic error (gray), the resulting overlap is 100% identical.*  
    4. The adjusted overlaps are then used to built draft contigs using Canu's Bogart module, modified to better handle heterozygous variants and consider only high-identity read overlaps  
    5. Final contigs are obttained by computing a consensus across the original, uncompressed reads, arranged according to the layout of their compressed versions. 

* Similar to many modern assemblers, when faced with a diploid genome, HiCanu outputs contigs as "pseudo-haplotypes" that preserve local allelic phasing but may switch between haplotypes across longer distances.
    * A single set of contigs representing all resolved alleles is output regardless of ploidy, and **additional processing** with a tool such as **Purge_dups** is required to partition the contigs into primary and alternate allele sets.  

## 3. *Drosophila* genome assembly  
* Assembly was tested in four conditions:  
    1. 40x coverage HiFi assembled with HiCanu  
    2. 40x coverage HiFi assembled with Peregrine
    3. 40x coverage HiFi assembled with Canu  
    4. 200x coverage CLR assembled with Canu  
    
* After assembly, contigs **less than 50 kbp** were **filtered** in order to exclude **low-quality** sequences consisting of only a **few reads**.  

* With exception of the Peregrine assembly, all other assemblies supported the hypothesis that alleles from both haplotypes were being represented in the final assembly. This was indicated by:  
    * The assembly length being more than twice that of the *Drosophila* haploid reference genome  
    * The large fraction of duplicated genes observed on BUSCO analysis  

* To allow like-for-like comparisons, all assemblies were processed with Purge_dups to separate primary and alternate contig sets. Assembly statistics were then calculated for both contig sets separately. The only analysis where the assembly used was the diploid one (i.e. before purge_dups) was the structural correctness, since purge_dups may introduce (or correct) mis-assemblies as it modifies the contigs.  

### 3.1 On the meaning of "Pseudo-haplotypes"  
* HiFi reads alone cannot be used to infer phasing across homozygous regions longer than the read length, so the contigs produced by HiCanu (and Canu) represent "pseudo-haplotypes", which may switch between haplotypes.  
* However, for highly heterozygous genomes with short regions of homozygosity, HiCanu is expected to produce a low number of haplotype switches and mostly preserve long-range phasing.  

### 3.2 On the meaning of "phase blocks"  
* We used Merqury to split the initial contigs into continuous phase blocks, based on haplotype-specific k-mer markers inferred from parental Illumina reads. 

## 4. Human genome assemblies  
* Three different libraries where tested, all consisting of approximetely 30x HiFi sequencing coverage:  
    1. 20 kbp library of the completely homozygous cell line CHM13  
    2. 15 kbp library of the Ashkenazic cell line HG002 (from the Personal Genome Project)  
    3. combined library (10 kbp, 15 kbp and 20 kbp fragments sizes) of the Puerto Rican cell line HG00733 (from the 1000 Genomes Project)     
* Then, for each library, three different assemblers were tested: HiCanu, Canu and Peregrine
* For each cell line tested, the most continuous published assembly was also considered in the comparative analysis of assembly strategies. Those assemblies relied on ultra-long Oxford Nanopore (ONT) reads to achieve state-of-the-art repeat resolution  
* As in the Drosophila evaluation, contigs less than 50 kbp were excluded from analysis and the diploid assemblies generated by HiCanu and Canu were broken into primary and alternate contig sets, that were evaluated separately.  

### 4.1 Continuity  
* The continuity of HiCanu assemblies, as measured by NG50, exceeded that of all other HiFi-based assemblies and even rivaled the continuity of Nanopore ultra-long read assemblies  
    * NG50 of CHM13 (just one set of contigs because CHM13 cells are completely homozygous) = 77.12 Mbp  
    * NG50 of HG002 (primary/alternate contig set) = 46.39 Mbp / 0.18 Mbp  
    * NG50 of HG00733 (primary/alternate contig set) = 38.43 Mbp / 0.23 Mbp

### 4.2 Consensus accuracy  
* For consensus accuracy, the HiCanu primary contig sets exceeded QV50 (99.999% accuracy) and alternate contigs sets exceeded QV40 (99.99% accuracy), while the unpolished Nanopore assemblies failed to exceed QV30 (99.9%).  

### 4.3 Structural accuracy  
* Reported rates of structural differences (between each assembly against current human reference genome GRCh38) for HiCanu was on pair with the other assemblies.  

### 4.4 Assembly size  
* The total length of the HiCanu alterante contig sets exceeded 2 Gbp, highlighting its ability to separate human alleles. As a comparison, the size of alternate contig sets across other assembly strategies did not exceed 400 Mbp. 

## 5. To polish or not to polish?  
* Although Nanopore assemblies currently require polishing with complementary technologies to maximize consensus accuracy, we discourage polishing HiCanu HiFi assemblies, because the available polishing pipelines may map reads back to the wrong repeat copies and actually introduce errors during polishing.

## 6. Human haplotype phasing  
* When assembling a diploid genome, an assembler must choose to either **collapse alleles** into a **single sequence** or **preserve** them as **two separate sequences**. Collapsing heterozygosity results in a **mosaic consensus** that may **not faithfully represent any allele** and can introduce **frameshifting errors** within coding sequence.  
* We again (as for *Drosophila*) used Merqury to analyze the phase blocks using parental Illumina data.  
* The phase block NG50s of HiCany primary (0.6 Mbp) and alternate (0.1 Mbp) contig sets were the highest across all considered assemblies
* Although the human phase blocks were significantly shorter than the ones from *Drosophila* (primary contigs NG50: 7.62 Mbp; alternate contigs NG50: 4.45 Mbp), they are longer than a typical human gene.  
* Recent studies have shown that **multi-megabase NG50 phase blocks** can be obtained by integrating HiFi data with **long-range linking information** derived from **Hi-C** or Strand-seq data.  

### 6.1 Assessing phasing accuracy  
* To assess the phasing accuracy the authors first made a variant calling of the haploid contig sets separately using the tool dipcall, and then compared the variants detected with the variants detected from a gold-standard variant set from the Genome in a Bottle (GIAB) consortium usign the tool vcfeval  
* Comparing sensitivity and precision of variant detection, the HiCanu assembly performed around 50% better than the next best HiFi assembly and similar to another assembly combining both HiFi and Hi-C data.  
* Finally, the human leukocyte antigen (HLA) genes alleles were investigated, what is interesting since HLA is a relevant gene complex (i.e. made by **neighboring** genes in a same chromosome). The HiCanu and TrioCanu assemblies were the only ones to recover all alleles of all genes with 100% identity. Even though HiCanu assembly is expected to have some contigs switches between haplotypes (since one HiFi read will not be capable to resolve long homozygous regions, as explained above), there was only one switch in the HLA region.  
* Interestingly, the assemblies of previous works that had used Hi-C or Strand-seq data besides PacBio but had used methods that collapsed haplotypes into a final haploid assembly showed some problems in the HLA genes, namely errors in the consensus sequence and incorrect representation of an allele as homozygous (missing a variant). This strengths the argument that **separation of haplotypes** early in the **assembly** process (as it is done by HiCanu) may improve the accurate **recovery of heterozygous** variation. 

## 7. Complex regions of the CHM13 human genome  

TO DO 

TO DO

# New tools to take a look  
* Merqury (Rhie et al. 2020) seems to be an interesting software to measure base-level accuracy of the assembly.   
* dipcall ([github here](https://github.com/lh3/dipcall)) a reference-based variant calling pipeline for a pair of phased haplotype assemblies.  
* vcfeval ([github here](https://github.com/RealTimeGenomics/rtg-tools))performs variant comparison at the haplotype level, that is, it determines whether the genotypes asserted in the VCFs under comparison result in the same genomic sequence when applied to the reference genome. 

# New data types to take a look  
* Strand-seq data as an option for long-range linking information (kind of like Hi-C)  

# Questions  
* Why are *Drosophila* NG50 phase blocks significantly higher than the ones obtained for humans? Does that mean that phasing worked better for the *Drosophila* dataset than for the human? If yes, why is that so? 

